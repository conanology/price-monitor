#!/usr/bin/env python3
"""
Price Monitoring Dashboard

Full-stack price tracking app. Scrapes product prices daily and visualizes trends. (Demo project)

Usage:
    python main.py --url <target_url>
"""

import argparse
import requests
from bs4 import BeautifulSoup
import pandas as pd
from pathlib import Path


def scrape_data(url):
    """
    Main scraping logic

    Args:
        url: Target URL to scrape

    Returns:
        pandas.DataFrame: Scraped data
    """
    # Set headers to avoid bot detection
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }

    response = requests.get(url, headers=headers)
    response.raise_for_status()

    soup = BeautifulSoup(response.content, 'html.parser')

    # TODO: Implement actual scraping logic
    # This is a template - customize per project

    data = []
    # Example: Extract items (customize selectors)
    # items = soup.select('.product-item')
    # for item in items:
    #     data.append({
    #         'name': item.select_one('.product-name').text.strip(),
    #         'price': item.select_one('.price').text.strip(),
    #     })

    return pd.DataFrame(data)


def main():
    parser = argparse.ArgumentParser(description='Price Monitoring Dashboard')
    parser.add_argument('--url', required=True, help='Target URL to scrape')
    parser.add_argument('--output', default='output/results.csv', help='Output file path')

    args = parser.parse_args()

    print(f"Scraping {args.url}...")
    df = scrape_data(args.url)

    # Save to CSV
    output_path = Path(args.output)
    output_path.parent.mkdir(exist_ok=True)
    df.to_csv(output_path, index=False)

    print(f"✅ Scraped {len(df)} items")
    print(f"✅ Saved to {output_path}")


if __name__ == '__main__':
    main()
